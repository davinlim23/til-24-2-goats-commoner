{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d227063-56a0-4bff-ac89-9e8d64ef390a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'others'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(os.path.join(\"advanced\", \"vlm.jsonl\"), lines=True)\n",
    "\n",
    "# build unique classes in dataset\n",
    "id2label = [\"missile\", \"cargo\", \"commercial\", \"light\", \"helicopter\", \"fighter\", \"drone\", \"others\"]\n",
    "label2id = {}\n",
    "for idx, label in enumerate(id2label):\n",
    "    label2id[label] = idx\n",
    "print(len(id2label))\n",
    "id2label[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282737f9-db19-4550-b7dc-ac77626c3f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Val images and labels\n",
    "from PIL import Image\n",
    "\n",
    "new_dir = \"datasets/vlm_detr\"\n",
    "old_dir = \"advanced\"\n",
    "dataset_type = \"val\"\n",
    "\n",
    "# Delete the directory if it exists\n",
    "if os.path.exists(os.path.join(new_dir, dataset_type)):\n",
    "    os.system(f\"rm -rf {os.path.join(new_dir, dataset_type)}\")\n",
    "\n",
    "# Create the coco format\n",
    "for idx, row in data.iterrows():\n",
    "    image_path = row[\"image\"]\n",
    "    annotations = row[\"annotations\"]\n",
    "    new_image_path = os.path.join(new_dir, dataset_type, 'images', image_path)\n",
    "    new_txt_path = new_image_path.replace(\"jpg\", \"txt\").replace(\"images\", \"labels\")\n",
    "    # Copy the image to the new directory\n",
    "    os.makedirs(os.path.dirname(new_image_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(new_txt_path), exist_ok=True)\n",
    "    os.system(f\"cp {os.path.join(old_dir, 'images', image_path)} {new_image_path}\")\n",
    "\n",
    "    # Get the size of the image\n",
    "    with Image.open(os.path.join(old_dir, \"images\", image_path)) as img:\n",
    "        width, height = img.size\n",
    "        # Since the image is not the actual image\n",
    "        width = 1520\n",
    "        height = 870\n",
    "\n",
    "    with open(new_txt_path, \"w\") as f:\n",
    "        for annotation in annotations:\n",
    "            caption = annotation[\"caption\"]\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            x, y, w, h = bbox\n",
    "            x_center = x + w / 2\n",
    "            y_center = y + h / 2\n",
    "            \n",
    "            class_id = label2id[\"others\"]\n",
    "            for word in caption.split(\" \"):\n",
    "                if word in label2id.keys():\n",
    "                    class_id = label2id[word]\n",
    "\n",
    "            # Normalize the values to be between 0 and 1\n",
    "            x_center /= width\n",
    "            y_center /= height\n",
    "            w /= width\n",
    "            h /= height\n",
    "            \n",
    "            f.write(f\"{class_id} {x_center} {y_center} {w} {h}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7595c3-af43-43f8-94d5-3d600eac0804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train images and labels\n",
    "from PIL import Image\n",
    "\n",
    "new_dir = \"datasets/vlm_detr\"\n",
    "old_dir = \"advanced\"\n",
    "dataset_type = \"train\"\n",
    "\n",
    "# Delete the directory if it exists\n",
    "if os.path.exists(os.path.join(new_dir, dataset_type)):\n",
    "    os.system(f\"rm -rf {os.path.join(new_dir, dataset_type)}\")\n",
    "\n",
    "# Create the coco format\n",
    "for idx, row in data.iterrows():\n",
    "    image_path = row[\"image\"]\n",
    "    annotations = row[\"annotations\"]\n",
    "    new_image_path = os.path.join(new_dir, dataset_type, 'images', image_path)\n",
    "    new_txt_path = new_image_path.replace(\"jpg\", \"txt\").replace(\"images\", \"labels\")\n",
    "    # Copy the image to the new directory\n",
    "    os.makedirs(os.path.dirname(new_image_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(new_txt_path), exist_ok=True)\n",
    "    os.system(f\"cp {os.path.join(old_dir, 'images', image_path)} {new_image_path}\")\n",
    "\n",
    "    # Get the size of the image\n",
    "    with Image.open(os.path.join(old_dir, \"images\", image_path)) as img:\n",
    "        width, height = img.size\n",
    "        # Since the image is not the actual image\n",
    "        width = 1520\n",
    "        height = 870\n",
    "\n",
    "    with open(new_txt_path, \"w\") as f:\n",
    "        for annotation in annotations:\n",
    "            caption = annotation[\"caption\"]\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            x, y, w, h = bbox\n",
    "            x_center = x + w / 2\n",
    "            y_center = y + h / 2\n",
    "            \n",
    "            class_id = label2id[\"others\"]\n",
    "            for word in caption.split(\" \"):\n",
    "                if word in label2id.keys():\n",
    "                    class_id = label2id[word]\n",
    "\n",
    "            # Normalize the values to be between 0 and 1\n",
    "            x_center /= width\n",
    "            y_center /= height\n",
    "            w /= width\n",
    "            h /= height\n",
    "            \n",
    "            f.write(f\"{class_id} {x_center} {y_center} {w} {h}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7d1390-701e-4c29-93ea-19d2908e70a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train: 5107\n",
      "Number of images in val: 5107\n",
      "Number of labels in train: 5107\n",
      "Number of labels in val: 5107\n"
     ]
    }
   ],
   "source": [
    "# Check if the files have been created\n",
    "train_dir = os.path.join(new_dir, \"train\")\n",
    "val_dir = os.path.join(new_dir, \"val\")\n",
    "train_dir_images = os.path.join(train_dir, \"images\")\n",
    "val_dir_images = os.path.join(val_dir, \"images\")\n",
    "\n",
    "train_dir_labels = os.path.join(train_dir, \"labels\")\n",
    "val_dir_labels = os.path.join(val_dir, \"labels\")\n",
    "\n",
    "print(f\"Number of images in train: {len(os.listdir(train_dir_images))}\")\n",
    "print(f\"Number of images in val: {len(os.listdir(val_dir_images))}\")\n",
    "\n",
    "print(f\"Number of labels in train: {len(os.listdir(train_dir_labels))}\")\n",
    "print(f\"Number of labels in val: {len(os.listdir(val_dir_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293cd62e-0c9a-40a4-9d58-c21eb7c1e3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_path_list = os.listdir(train_dir_images) \n",
    "\n",
    "for x in images_path_list:\n",
    "    path = os.path.join(train_dir_images, x)\n",
    "    image = Image.open(path)\n",
    "    if image.size != (1520, 870):\n",
    "        print(\"No right size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa43edfa-46b2-4c0c-8ac9-e51499c4fd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add data.yaml file into data directory\n",
    "# Format of the data.yaml file\n",
    "\"\"\"\n",
    "train: ../train/images\n",
    "val: ../val/images\n",
    "\n",
    "nc: number of classes\n",
    "names: [class1, class2, class3, ...]\n",
    "\"\"\"\n",
    "\n",
    "data_yaml_path = os.path.join(new_dir, \"data.yaml\")\n",
    "\n",
    "with open(data_yaml_path, \"w\") as f:\n",
    "    f.write(f\"train: ../train/images\\n\")\n",
    "    f.write(f\"val: ../val/images\\n\\n\")\n",
    "    f.write(f\"nc: {len(id2label)}\\n\")\n",
    "    f.write(f\"names: {id2label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614e0851-a43f-4974-b8dd-864c03f45beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "from ultralytics.data.augment import Albumentations\n",
    "from ultralytics.utils import LOGGER, colorstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29790971-52c1-482e-a470-5f909b87867f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model_path = os.path.join(\"runs\", \"detect\", \"train93\", \"weights\", \"best.pt\")\n",
    "model = RTDETR(\"models/detr/rtdetr-x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60542d2-c6dc-4800-b731-5b86965257c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/vlm_detr/data.yaml'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dir = \"datasets/vlm_detr\"\n",
    "data_yaml_path = os.path.join(new_dir, \"data.yaml\")\n",
    "data_yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3124c2-4b1f-4dc0-888e-4ddfe5c52f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def __init__(self, p=1.0):\n",
    "        \"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\n",
    "        self.p = p\n",
    "        self.transform = None\n",
    "        prefix = colorstr(\"albumentations: \")\n",
    "        try:\n",
    "            import albumentations as A\n",
    "            \n",
    "            spatial_transforms = {\n",
    "                \"Affine\",\n",
    "                \"BBoxSafeRandomCrop\",\n",
    "                \"CenterCrop\",\n",
    "                \"CoarseDropout\",\n",
    "                \"Crop\",\n",
    "                \"CropAndPad\",\n",
    "                \"CropNonEmptyMaskIfExists\",\n",
    "                \"D4\",\n",
    "                \"ElasticTransform\",\n",
    "                \"Flip\",\n",
    "                \"GridDistortion\",\n",
    "                \"GridDropout\",\n",
    "                \"HorizontalFlip\",\n",
    "                \"Lambda\",\n",
    "                \"LongestMaxSize\",\n",
    "                \"MaskDropout\",\n",
    "                \"MixUp\",\n",
    "                \"Morphological\",\n",
    "                \"NoOp\",\n",
    "                \"OpticalDistortion\",\n",
    "                \"PadIfNeeded\",\n",
    "                \"Perspective\",\n",
    "                \"PiecewiseAffine\",\n",
    "                \"PixelDropout\",\n",
    "                \"RandomCrop\",\n",
    "                \"RandomCropFromBorders\",\n",
    "                \"RandomGridShuffle\",\n",
    "                \"RandomResizedCrop\",\n",
    "                \"RandomRotate90\",\n",
    "                \"RandomScale\",\n",
    "                \"RandomSizedBBoxSafeCrop\",\n",
    "                \"RandomSizedCrop\",\n",
    "                \"Resize\",\n",
    "                \"Rotate\",\n",
    "                \"SafeRotate\",\n",
    "                \"ShiftScaleRotate\",\n",
    "                \"SmallestMaxSize\",\n",
    "                \"Transpose\",\n",
    "                \"VerticalFlip\",\n",
    "                \"XYMasking\",\n",
    "            }  # from https://albumentations.ai/docs/getting_started/transforms_and_targets/#spatial-level-transforms\n",
    "\n",
    "            # Insert required transformation here\n",
    "            T = [\n",
    "                # A.RandomRain(p=0.4, slant_lower=-10, slant_upper=10, \n",
    "                #               drop_length=20, drop_width=1, drop_color=(200, 200, 200), \n",
    "                #               blur_value=5, brightness_coefficient=0.9, rain_type=None),\n",
    "                # A.Rotate(limit = 10, p=0.5),\n",
    "                # A.Blur(p=0.1),\n",
    "                # A.HorizontalFlip(p=0.5),  # Adds horizontal flipping with a 50% probability\n",
    "                # A.VerticalFlip(p=0.5),    # Adds \n",
    "                # A.MedianBlur(p=0.1),\n",
    "                # A.ImageCompression(quality_lower=75, p=0.0),\n",
    "                A.Rotate(limit=15, p=0.3),\n",
    "                A.Blur(blur_limit=(3, 5), p=0.3),\n",
    "                A.RandomSizedCrop(min_max_height=(int(0.8 * 640), 640), height=640, width=640, p=0.3)\n",
    "            ]\n",
    "\n",
    "            self.contains_spatial = any(transform.__class__.__name__ in spatial_transforms for transform in T)\n",
    "            self.transform = (\n",
    "                A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n",
    "                if self.contains_spatial\n",
    "                else A.Compose(T)\n",
    "            )\n",
    "            \n",
    "            LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
    "        except ImportError:  # package not installed, skip\n",
    "            print(\"Importing error\")\n",
    "        except Exception as e:\n",
    "            LOGGER.info(f\"{prefix}{e}\")\n",
    "\n",
    "Albumentations.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbfa3f-fc2a-4e55-ae6a-e07c193618b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.27 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.22 🚀 Python-3.10.14 torch-1.13.1+cu117 CUDA:0 (Tesla T4, 14918MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=models/detr/rtdetr-x.pt, data=datasets/vlm_detr/data.yaml, epochs=100, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=models/detr, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=models/detr/train6\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "WARNING ⚠️ no model scale passed. Assuming scale='x'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25792  ultralytics.nn.modules.block.HGStem          [3, 32, 64]                   \n",
      "  1                  -1  6    259200  ultralytics.nn.modules.block.HGBlock         [64, 64, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6   1248256  ultralytics.nn.modules.block.HGBlock         [128, 128, 512, 3, 6]         \n",
      "  4                  -1  6   1788928  ultralytics.nn.modules.block.HGBlock         [512, 128, 512, 3, 6, False, True]\n",
      "  5                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  6                  -1  6   2079232  ultralytics.nn.modules.block.HGBlock         [512, 256, 1024, 5, 6, True, False]\n",
      "  7                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n",
      "  8                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n",
      "  9                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n",
      " 10                  -1  6   2472448  ultralytics.nn.modules.block.HGBlock         [1024, 256, 1024, 5, 6, True, True]\n",
      " 11                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      " 12                  -1  6   8221696  ultralytics.nn.modules.block.HGBlock         [1024, 512, 2048, 5, 6, True, False]\n",
      " 13                  -1  6   9794560  ultralytics.nn.modules.block.HGBlock         [2048, 512, 2048, 5, 6, True, True]\n",
      " 14                  -1  1    787200  ultralytics.nn.modules.conv.Conv             [2048, 384, 1, 1, None, 1, 1, False]\n",
      " 15                  -1  1   2168192  ultralytics.nn.modules.transformer.AIFI      [384, 2048, 8]                \n",
      " 16                  -1  1    148224  ultralytics.nn.modules.conv.Conv             [384, 384, 1, 1]              \n",
      " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 18                  10  1    393984  ultralytics.nn.modules.conv.Conv             [1024, 384, 1, 1, None, 1, 1, False]\n",
      " 19            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n",
      " 21                  -1  1    148224  ultralytics.nn.modules.conv.Conv             [384, 384, 1, 1]              \n",
      " 22                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 23                   4  1    197376  ultralytics.nn.modules.conv.Conv             [512, 384, 1, 1, None, 1, 1, False]\n",
      " 24            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 25                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n",
      " 26                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 27            [-1, 21]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 28                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n",
      " 29                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 30            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 31                  -1  3   5019648  ultralytics.nn.modules.block.RepC3           [768, 384, 3]                 \n",
      " 32        [25, 28, 31]  1   7416596  ultralytics.nn.modules.head.RTDETRDecoder    [8, [384, 384, 384]]          \n",
      "rt-detr-x summary: 867 layers, 67319892 parameters, 67319892 gradients, 232.4 GFLOPs\n",
      "\n",
      "Transferred 1226/1241 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir models/detr/train6', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/datasets/vlm_detr/train/labels... 5107 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5107/5107 [00:04<00:00, 1039.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jupyter/datasets/vlm_detr/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mRotate(p=0.3, limit=(-15, 15), interpolation=1, border_mode=4, value=None, mask_value=None), Blur(p=0.3, blur_limit=(3, 5)), RandomSizedCrop(p=0.3, min_max_height=(512, 640), height=640, width=640, w2h_ratio=1.0, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/datasets/vlm_detr/val/labels... 5107 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5107/5107 [00:04<00:00, 1064.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jupyter/datasets/vlm_detr/val/labels.cache\n",
      "Plotting labels to models/detr/train6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 193 weight(decay=0.0), 256 weight(decay=0.0005), 276 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mmodels/detr/train6\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      1/100        12G      0.565      2.384      0.096          6        640: 100%|██████████| 639/639 [08:45<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:26<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.776      0.671      0.652      0.484\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      2/100      11.9G     0.4523     0.5935    0.06132         14        640: 100%|██████████| 639/639 [08:23<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:27<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.916      0.849      0.866      0.646\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      3/100      11.9G     0.4342     0.5479     0.0568         18        640: 100%|██████████| 639/639 [08:17<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:26<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.928      0.875       0.89      0.607\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      4/100      11.9G     0.4229     0.5306    0.05391         21        640: 100%|██████████| 639/639 [08:13<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:26<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.943      0.893      0.915      0.636\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      5/100        12G     0.4093     0.5028    0.05221         27        640: 100%|██████████| 639/639 [08:10<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:26<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.928        0.9      0.927      0.717\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      6/100        12G      0.401      0.502    0.05097         14        640: 100%|██████████| 639/639 [08:11<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:26<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.949      0.888      0.918      0.707\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      7/100      11.9G     0.3933     0.4789    0.04979         18        640: 100%|██████████| 639/639 [08:14<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:29<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.962      0.933      0.948       0.73\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      8/100      11.9G      0.391     0.4667    0.04955         27        640: 100%|██████████| 639/639 [08:12<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:29<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.974      0.942      0.958      0.749\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "      9/100      11.9G     0.3866     0.4617    0.04828         16        640: 100%|██████████| 639/639 [08:11<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:30<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.967      0.942      0.955      0.725\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "     10/100        12G     0.3817     0.4568    0.04832         20        640: 100%|██████████| 639/639 [08:15<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:47<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913      0.949      0.923      0.945      0.711\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "     11/100      11.9G     0.3793     0.4558    0.04728         24        640: 100%|██████████| 639/639 [08:24<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 320/320 [02:34<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5107      27913        nan      0.969      0.961      0.747\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/639 [00:00<?, ?it/s]/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "     12/100      11.9G     0.3704     0.4371    0.04543         27        640:  31%|███       | 199/639 [02:33<05:47,  1.27it/s]"
     ]
    }
   ],
   "source": [
    "# Specify the save directory for training runs\n",
    "save_dir = os.path.join(\"models\", \"detr\", \"logs\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "results = model.train(data=data_yaml_path, epochs=100, imgsz=640, augment=True, batch=8, project=\"models/detr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def6442-5bcb-4a27-835a-c6cea257d409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conduct model validation on the COCO8 example dataset\n",
    "metrics = model.val(data=data_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757e558e-f9f5-48d5-b1ad-0bd729421638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jupyter/advanced/images/image_0.jpg: 384x640 1 blue, yellow, and green fighter plane, 1 grey and white light aircraft, 1 white and blue fighter jet, 1 yellow, red, and blue fighter plane, 1 black and white missile, 1 grey and yellow fighter plane, 1 white and red fighter jet, 19.6ms\n",
      "Speed: 18.4ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: 882: www-browser: not found\n",
      "/usr/bin/xdg-open: 882: links2: not found\n",
      "/usr/bin/xdg-open: 882: elinks: not found\n",
      "/usr/bin/xdg-open: 882: links: not found\n",
      "/usr/bin/xdg-open: 882: lynx: not found\n",
      "/usr/bin/xdg-open: 882: w3m: not found\n",
      "xdg-open: no method available for opening '/var/tmp/tmptb9y45dh.PNG'\n"
     ]
    }
   ],
   "source": [
    "test_image_path = os.path.join(\"advanced\", \"images\", \"image_0.jpg\")\n",
    "\n",
    "results = model.predict(test_image_path)\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "743ff670-c5be-46e0-9b1e-474c88fc2a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "\n",
    "bboxes_tensors = results[0].boxes.xywh\n",
    "for tensor in bboxes_tensors:\n",
    "    bboxes.append(tensor.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b9e5bc6-624c-457b-9974-e873f50d6a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([     576.76,      137.63,      99.661,      41.889], dtype=float32),\n",
       " array([        578,      138.45,      98.336,      40.679], dtype=float32),\n",
       " array([     1044.1,      93.673,      30.411,      34.963], dtype=float32),\n",
       " array([     1044.1,      93.753,      30.103,      34.893], dtype=float32),\n",
       " array([     742.72,      538.83,      79.015,      65.385], dtype=float32),\n",
       " array([     580.46,      621.92,      17.883,      17.654], dtype=float32),\n",
       " array([     743.31,      538.85,      80.516,        67.6], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b06c1ad3-6001-4922-9b4a-8c61e9bb65c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: 882: www-browser: not found\n",
      "/usr/bin/xdg-open: 882: links2: not found\n",
      "/usr/bin/xdg-open: 882: elinks: not found\n",
      "/usr/bin/xdg-open: 882: links: not found\n",
      "/usr/bin/xdg-open: 882: lynx: not found\n",
      "/usr/bin/xdg-open: 882: w3m: not found\n",
      "xdg-open: no method available for opening '/var/tmp/tmpspw2nzaq.PNG'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "\n",
    "image = Image.open(test_image_path)\n",
    "\n",
    "# Create a draw object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for bbox in bboxes:\n",
    "# Extract the center coordinates, width, and height\n",
    "    x_center, y_center, width, height = bbox\n",
    "\n",
    "    x1 = x_center - width / 2\n",
    "    y1 = y_center - height / 2\n",
    "    x2 = x_center + width / 2\n",
    "    y2 = y_center + height / 2\n",
    "\n",
    "\n",
    "    # Draw the rectangle\n",
    "    draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "\n",
    "# Display the image (if using Jupyter Notebook)\n",
    "image.show()\n",
    "\n",
    "# If not in a notebook, save or display the image as needed\n",
    "image.save('output_image_with_bbox_2.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
