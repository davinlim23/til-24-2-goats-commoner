{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d569fbaa-7d0d-460a-af79-48ba1b86ef2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "#---------------------------- Dependencies ---------------------------------------#\n",
    "import transformers\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration,WhisperProcessor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cf724f-3437-4031-9a92-873b5b061600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1485777-7b1f-4ced-9db9-dbb66419cc56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55403bf6-1ec7-474d-9261-d14a94d7e22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# if os.path.isdir(\"results\"):\n",
    "#     shutil.rmtree(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953e7389-69cc-44e8-8ddd-2e484b42a095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "from datasets import Dataset as hf_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1207a35-3827-44d5-b209-58ece6459844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1c3b33-64a3-4f6b-b264-df68fee29fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc9119d-921f-4501-a43e-1c6b7ae0fedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "\"\"\"\n",
    "Process file path for actual data path\n",
    "\"\"\"\n",
    "def process_data_files(data_path):\n",
    "    file_list = os.listdir(data_path)\n",
    "    # total_length = len(os.listdir(data_path))\n",
    "    new_list = [None] * len(file_list)\n",
    "    for i in range(len(file_list)):\n",
    "        corrected_file = f'{file_list[i][0:6]}{i}.wav'\n",
    "        new_list[i] = os.path.join(data_path,corrected_file)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "\"\"\"\n",
    "Manual Process Json metadata file \n",
    "\"\"\"\n",
    "def process_json(annotation_path:str) -> dict:\n",
    "    json_dict = {}\n",
    "    with open (annotation_path,'r') as json_file:\n",
    "        json_data = list(json_file)\n",
    "    for idx,line in enumerate(json_data):\n",
    "        json_dict[idx] = line\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "\"\"\"\n",
    "Process and resample the audio\n",
    "\"\"\"\n",
    "def process_audio(self,waveform,original_sample_rate,new_sample_rate=16000):\n",
    "    waveform = torch.Tensor(waveform)\n",
    "    if original_sample_rate != new_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=new_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        print(type(waveform))\n",
    "        print(f\"Resampled waveform to {new_sample_rate} Hz\")\n",
    "    # Normalize audio to [-1, 1]\n",
    "    # waveform = waveform / torch.max(torch.abs(waveform))\n",
    "    return waveform,new_sample_rate\n",
    "\n",
    "def denoise_data(audio,rate):\n",
    "    # Perform noise reduction\n",
    "    noisy_part = audio[0:int(rate*0.5)]  # Identify the noisy part\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio, sr=rate, y_noise=noisy_part)\n",
    "    return reduced_noise_audio\n",
    "\n",
    "# Function to load and preprocess audio\n",
    "def preprocess_data(examples):\n",
    "    input_values = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for audio_path, transcript in zip(examples['data'], examples['annotations']):\n",
    "        speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        processed = processor(speech_array.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Process labels with the same processor settings\n",
    "        with processor.as_target_processor():\n",
    "            label = processor(transcript, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        input_values.append(processed.input_values.squeeze(0))\n",
    "        # Create attention masks based on the input values\n",
    "        attention_mask = torch.ones_like(processed.input_values)\n",
    "        attention_mask[processed.input_values == processor.tokenizer.pad_token_id] = 0  # Set padding tokens to 0\n",
    "        attention_masks.append(attention_mask.squeeze(0))\n",
    "        \n",
    "        # Ensure labels are padded to the same length as inputs if needed\n",
    "        padded_label = torch.full(processed.input_values.shape[1:], -100, dtype=torch.long)\n",
    "        actual_length = label.input_ids.shape[1]\n",
    "        padded_label[:actual_length] = label.input_ids.squeeze(0)\n",
    "        labels.append(padded_label)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    examples['input_values'] = torch.stack(input_values)\n",
    "    examples['attention_mask'] = torch.stack(attention_masks)\n",
    "    examples['labels'] = torch.stack(labels)\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a87d593-780e-4f9f-b443-546c0785e13e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [10:12<00:00,  5.71it/s]\n"
     ]
    }
   ],
   "source": [
    "annotation_path = 'advanced/asr.jsonl'\n",
    "\n",
    "# Pandas Way\n",
    "json_files = pd.read_json(annotation_path,lines=True)\n",
    "\n",
    "annotations_list = json_files['transcript'].to_list()\n",
    "file_list = process_data_files('advanced/audio')\n",
    "\n",
    "audio_features = []\n",
    "for file in tqdm(file_list):\n",
    "    audio_wave, sample_rate = torchaudio.load(file)\n",
    "    audio_wave = denoise_data(audio_wave,sample_rate)\n",
    "    # target,new_sample_rate = process_audio(audio_wave,sample_rate,16000)\n",
    "    audio_features.append({'waveform': audio_wave, 'sampling_rate':sample_rate })\n",
    "\n",
    "# model_name = \"openai/whisper-small\"\n",
    "# processor = WhisperProcessor.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "# feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41c8c490-cf60-4aec-9784-c92d317ecfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(audio_features, annotations_list, test_size=0.3, random_state=1)\n",
    "\n",
    "# X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5, random_state=1) \n",
    "\n",
    "# len(X_train),len(X_test),len(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3201ff-86d3-4874-9b23-89356a251b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = hf_dataset.from_dict({'data':X_train, 'annotations': y_train})\n",
    "# val_dataset = hf_dataset.from_dict({'data':X_val, 'annotations': y_val})\n",
    "test_dataset = hf_dataset.from_dict({'data':X_test, 'annotations': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354524a6-aa5d-4d9f-a122-88bb70c8e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crashes the kernel\n",
    "\n",
    "# dataset_dict = \\\n",
    "# {\n",
    "#     'data':audio_features,\n",
    "#     'annotations': annotations_list\n",
    "# }\n",
    "\n",
    "# # Convert to a Hugging Face dataset\n",
    "# dataset = hf_dataset.from_dict(dataset_dict)\n",
    "\n",
    "\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = int(0.1 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# train_dataset = dataset.select(range(train_size))\n",
    "# val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "# test_dataset = dataset.select(range(train_size + val_size, train_size + val_size + test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb2e8d8-70e6-4035-b968-b38c71368210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data', 'annotations'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae93dac-5264-429d-94dd-d89737f79f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"openai/whisper-small\"\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"English\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23ac6f0-c1e0-47ca-a810-0aa07a45ab96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset prep functions\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "   # load audio data\n",
    "    audio = batch[\"data\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"waveform\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    \n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"annotations\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "def augment_dataset(batch):\n",
    "    # load and (possibly) resample audio data to 16kHz\n",
    "    audio = batch[\"data\"]\n",
    "\n",
    "    # apply augmentation\n",
    "    augmented_waveform = augmentation(audio[\"waveform\"], sample_rate=audio[\"sampling_rate\"])\n",
    "    batch[\"data\"][\"waveform\"] = augmented_waveform\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b59d4e-f0b4-4b87-a4c0-85c92b40179a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a39f6cb8954ac7a78c3ed4c1480b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cd0941dc2c4d1fa606cf61d885f070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset,remove_columns=train_dataset.column_names)\n",
    "# val_dataset = val_dataset.map(prepare_dataset,remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(prepare_dataset,remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1abdec-6f90-49fd-afb8-2734bc9bfd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b662cea2adc4b158527afa364507316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2758809bc9fe4547ab7f9966cc7ffb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "train_dataset.save_to_disk(\"audio_train_denoised.hf\")\n",
    "# val_dataset.save_to_disk(\"audio_val_denoised.hf\")\n",
    "test_dataset.save_to_disk(\"audio_test_denoised.hf\")\n",
    "\n",
    "\n",
    "# train_dataset.save_to_disk(\"audio_train.hf\")\n",
    "# val_dataset.save_to_disk(\"audio_val.hf\")\n",
    "# test_dataset.save_to_disk(\"audio_test.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2311a94-9764-43d5-a53a-37c41e8a0cc1",
   "metadata": {},
   "source": [
    "<h1> Model Training Phase </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45f415f8-81b1-48b3-8dd7-c49529bcf059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"audio_train_denoised.hf\")\n",
    "val_dataset = load_from_disk(\"audio_val_denoised.hf\")\n",
    "test_dataset = load_from_disk(\"audio_test_denoised.hf\")\n",
    "\n",
    "# train_dataset = load_from_disk(\"audio_train.hf\")\n",
    "# val_dataset = load_from_disk(\"audio_val.hf\")\n",
    "# test_dataset = load_from_disk(\"audio_test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8fabad-d84b-425d-88ab-c81b6b431f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 2450\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05626ab-901a-4ab6-9b65-0eb2e9fe36e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Taken from https://huggingface.co/blog/fine-tune-whisper\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4c27fd-2c5a-4169-98e6-4d534262526b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063cab99-2888-476d-b5d8-18e817914d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b38c1b6-77cd-4614-ae29-d70fa21051f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09f846f1-cc9f-4f6a-a02d-4b2b28d838d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not tuned \n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments,EarlyStoppingCallback\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps = 1,\n",
    "    # warmup_steps=500,\n",
    "    # max_steps=5000,\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs = 5,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16 = True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy = \"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    # push_to_hub=True,\n",
    "    save_safetensors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "106a4ca4-2763-4e62-b7e2-cb63fac34e20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3065' max='3065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3065/3065 4:22:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>4.517054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.059806</td>\n",
       "      <td>4.568268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>3.943460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.054570</td>\n",
       "      <td>3.871761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.058693</td>\n",
       "      <td>4.209772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>3.800061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.062116</td>\n",
       "      <td>3.912732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.058827</td>\n",
       "      <td>4.035645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.052568</td>\n",
       "      <td>3.318652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.053094</td>\n",
       "      <td>3.666906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.051713</td>\n",
       "      <td>3.482536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>3.400594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>3.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.057655</td>\n",
       "      <td>3.165011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.059474</td>\n",
       "      <td>3.226467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.059332</td>\n",
       "      <td>3.482536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.059581</td>\n",
       "      <td>3.359623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.062201</td>\n",
       "      <td>3.841032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.061385</td>\n",
       "      <td>3.533750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.069812</td>\n",
       "      <td>3.533750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>3.185496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>3.656663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.069633</td>\n",
       "      <td>3.287924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.074242</td>\n",
       "      <td>3.287924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>3.298167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.077218</td>\n",
       "      <td>3.175254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.078929</td>\n",
       "      <td>3.195739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.078921</td>\n",
       "      <td>3.236710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.079595</td>\n",
       "      <td>3.205982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.079826</td>\n",
       "      <td>3.308409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3065, training_loss=0.014581798161748004, metrics={'train_runtime': 15737.6645, 'train_samples_per_second': 0.778, 'train_steps_per_second': 0.195, 'total_flos': 3.53517115392e+18, 'train_loss': 0.014581798161748004, 'epoch': 5.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args = training_args,\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer= processor.feature_extractor\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee1c2c8d-50d1-42ba-bd2d-439756831e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('models/best_ASR_model_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67008d1-8d4d-4848-a21c-2f8d6345cb0d",
   "metadata": {},
   "source": [
    "<h1> INFERENCE CODE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f936a93-6d7e-4b9d-ab47-d936fd87eea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'til-24-base/asr/src/best_ASR_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'til-24-base/asr/src/best_ASR_model'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m processor(waveform\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), sampling_rate \u001b[38;5;241m=\u001b[39m sample_rate,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_features\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_features\n\u001b[0;32m---> 17\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtil-24-base/asr/src/best_ASR_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m processor \u001b[38;5;241m=\u001b[39m WhisperProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WhisperTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 2928\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2938\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2940\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2942\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py:450\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'til-24-base/asr/src/best_ASR_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "def denoise_data(audio,rate):\n",
    "    # Perform noise reduction\n",
    "    noisy_part = audio[0:int(rate*0.5)]  # Identify the noisy part\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio, sr=rate, y_noise=noisy_part)\n",
    "    return reduced_noise_audio\n",
    "\n",
    "def prepare_for_inference(audio_path,processor):\n",
    "   # load audio data\n",
    "    waveform,sample_rate = torchaudio.load(audio_path)\n",
    "    print(waveform.shape)\n",
    "    waveform = denoise_data(waveform,sample_rate)\n",
    "    # compute log-Mel input features from input audio array \n",
    "    input_features = processor(waveform.squeeze(0), sampling_rate = sample_rate,return_tensors=\"pt\").input_features\n",
    "    \n",
    "    return input_features\n",
    "\n",
    "new_model = WhisperForConditionalGeneration.from_pretrained('til-24-base/asr/src/best_ASR_model')\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n",
    "new_model.generation_config.forced_decoder_ids = None\n",
    "new_model.to(device)\n",
    "\n",
    "to_be_predicted = prepare_for_inference('advanced/audio/audio_0.wav',processor).to(device)\n",
    "\n",
    "prediction_ids = new_model.generate(to_be_predicted)\n",
    "\n",
    "transcription = processor.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fde94-7920-437f-81d0-738710851342",
   "metadata": {},
   "source": [
    "\n",
    "<b>Currently Obsolete Code<b>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379efe76-afc4-424b-bd76-b69d608bf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self,data_list,annotations_list,processor,tokenizer):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.transforms = transforms\n",
    "        self.data_dict = \\\n",
    "        {\n",
    "            'data':data_list,\n",
    "            'annotations':annotations_list\n",
    "        }\n",
    "        self.dataset = hf_dataset.from_dict(self.data_dict)\n",
    "            \n",
    "    \"\"\"\n",
    "    Resample if sample rate not 16000\n",
    "    \"\"\"\n",
    "    def process_audio(self,waveform,original_sample_rate,new_sample_rate=16000):\n",
    "        if waveform.shape[1] != new_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=new_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            print(f\"Resampled waveform to {new_sample_rate} Hz\")\n",
    "        # Normalize audio to [-1, 1]\n",
    "        waveform = waveform / torch.max(torch.abs(waveform))\n",
    "        return waveform , new_sample_rate     \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict['data'])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        file_path = self.data_dict['data'][index]\n",
    "        label = self.data_dict['annotations'][index]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        # new_waveform, new_sample_rate = self.process_audio(waveform,original_sample_rate = sample_rate,new_sample_rate = 16000)\n",
    "        target = self.preprocess_data(waveform,sample_rate,label)\n",
    "\n",
    "        return target\n",
    "    \n",
    "    \n",
    "        # Function to load and preprocess audio\n",
    "    def preprocess_data(self,speech_array,sampling_rate,text):\n",
    "        target = {}\n",
    "        processed = self.processor(speech_array.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "        # Process labels with the same processor settings\n",
    "        with self.processor.as_target_processor():\n",
    "            label = self.processor(text, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "        input_values = processed.input_values.squeeze(0)\n",
    "        # Create attention masks based on the input values\n",
    "        attention_mask = torch.ones_like(processed.input_values)\n",
    "        attention_mask[processed.input_values == self.processor.tokenizer.pad_token_id] = 0  # Set padding tokens to 0\n",
    "        attention_masks = attention_mask.squeeze(0)\n",
    "\n",
    "        # Ensure labels are padded to the same length as inputs if needed\n",
    "        padded_label = torch.full(processed.input_values.shape[1:], -100, dtype=torch.long)\n",
    "        actual_length = label.input_ids.shape[1]\n",
    "        padded_label[:actual_length] = label.input_ids.squeeze(0)\n",
    "        labels = padded_label\n",
    "\n",
    "        # Concatenate all batches\n",
    "        target['input_values'] = input_values\n",
    "        target['attention_mask'] = attention_masks\n",
    "        target['labels'] = labels\n",
    "\n",
    "        return target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd1f51-65f8-4123-af47-f9bee7419c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperModelWrapper(self,model,device,weights):\n",
    "    \n",
    "    self.model = model\n",
    "    self.device = device\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191693-d9f2-4726-aa1a-97e471f2079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c9e87-d5ff-496c-aad3-af713e7c3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd46bb-f280-4253-b0fa-c37710dbc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Train Loop\n",
    "audio_dataset = ASRDataset(data_list = file_list, annotations_list = annotations_list,processor = processor,tokenizer = tokenizer)\n",
    "\n",
    "main_dataset = audio_dataset.dataset\n",
    "\n",
    "print(main_dataset)\n",
    "# Shuffle the dataset\n",
    "dataset = main_dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, train_size + val_size + test_size))\n",
    "\n",
    "\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(audio_dataset, batch_size=2)\n",
    "\n",
    "# print(audio_dataset.__getitem__(50))\n",
    "\n",
    "\n",
    "# # validation\n",
    "# print(\"-------Actual data-------\")\n",
    "# print(file_list[50])\n",
    "# print(json_files['audio'][50])\n",
    "# print(annotations_list[50])\n",
    "\n",
    "# print(\"-----Dataset Class------\")\n",
    "# print(audio_dataset.data_dict['data'][50])\n",
    "# print(audio_dataset.data_dict['annotations'][50])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99a1a7-5549-4862-a866-43d6e0514491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     learning_rate=1e-4,\n",
    "#     per_device_train_batch_size=1,  # Reduce to one for simplicity\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.005,\n",
    "#     save_steps=500,\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=10,\n",
    "#     load_best_model_at_end=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d319b9-6ad0-46ee-a1b4-18449103d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,  # Use the validation dataset for evaluation\n",
    "#     tokenizer=processor.feature_extractor\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
