{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758fa9af-760e-4fed-9a17-1e178148ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d997a735-e638-42b0-b547-f4f0c89ab097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5dd054cd70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0163f0-c9c2-4963-bc36-4f4b243dff98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0.jpg</td>\n",
       "      <td>[{'caption': 'grey missile', 'bbox': [912, 164, 48, 152]}, {'caption': 'red, white, and blue light aircraft', 'bbox': [1032, 80, 24, 28]}, {'caption': 'green and black missile', 'bbox': [704, 508, 76, 64]}, {'caption': 'white and red helicopter', 'bbox': [524, 116, 112, 48]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>[{'caption': 'grey camouflage fighter jet', 'bbox': [1112, 172, 64, 36]}, {'caption': 'grey and white fighter plane', 'bbox': [1108, 512, 144, 48]}, {'caption': 'white and black drone', 'bbox': [356, 452, 48, 32]}, {'caption': 'white and black fighter jet', 'bbox': [404, 156, 48, 36]}, {'caption': 'white missile', 'bbox': [544, 112, 40, 40]}, {'caption': 'black and white commercial aircraft', 'bbox': [808, 504, 68, 68]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_2.jpg</td>\n",
       "      <td>[{'caption': 'grey drone', 'bbox': [552, 296, 56, 52]}, {'caption': 'white and black drone', 'bbox': [992, 504, 92, 48]}, {'caption': 'yellow, red, and grey helicopter', 'bbox': [304, 88, 56, 32]}, {'caption': 'yellow commercial aircraft', 'bbox': [808, 464, 76, 60]}, {'caption': 'black cargo aircraft', 'bbox': [948, 96, 44, 36]}, {'caption': 'yellow helicopter', 'bbox': [452, 108, 40, 36]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_3.jpg</td>\n",
       "      <td>[{'caption': 'white and black light aircraft', 'bbox': [476, 324, 44, 44]}, {'caption': 'grey and black fighter plane', 'bbox': [760, 260, 56, 40]}, {'caption': 'yellow helicopter', 'bbox': [984, 500, 108, 44]}, {'caption': 'red fighter plane', 'bbox': [1016, 324, 208, 68]}, {'caption': 'yellow, red, and grey helicopter', 'bbox': [680, 340, 72, 44]}, {'caption': 'white missile', 'bbox': [1016, 176, 40, 48]}, {'caption': 'blue helicopter', 'bbox': [496, 512, 44, 40]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_4.jpg</td>\n",
       "      <td>[{'caption': 'white, red, and green fighter plane', 'bbox': [260, 232, 60, 56]}, {'caption': 'black camouflage fighter jet', 'bbox': [640, 172, 36, 32]}, {'caption': 'green light aircraft', 'bbox': [632, 328, 76, 48]}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image  \\\n",
       "0  image_0.jpg   \n",
       "1  image_1.jpg   \n",
       "2  image_2.jpg   \n",
       "3  image_3.jpg   \n",
       "4  image_4.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                               annotations  \n",
       "0                                                                                                                                                                                                     [{'caption': 'grey missile', 'bbox': [912, 164, 48, 152]}, {'caption': 'red, white, and blue light aircraft', 'bbox': [1032, 80, 24, 28]}, {'caption': 'green and black missile', 'bbox': [704, 508, 76, 64]}, {'caption': 'white and red helicopter', 'bbox': [524, 116, 112, 48]}]  \n",
       "1                                                 [{'caption': 'grey camouflage fighter jet', 'bbox': [1112, 172, 64, 36]}, {'caption': 'grey and white fighter plane', 'bbox': [1108, 512, 144, 48]}, {'caption': 'white and black drone', 'bbox': [356, 452, 48, 32]}, {'caption': 'white and black fighter jet', 'bbox': [404, 156, 48, 36]}, {'caption': 'white missile', 'bbox': [544, 112, 40, 40]}, {'caption': 'black and white commercial aircraft', 'bbox': [808, 504, 68, 68]}]  \n",
       "2                                                                               [{'caption': 'grey drone', 'bbox': [552, 296, 56, 52]}, {'caption': 'white and black drone', 'bbox': [992, 504, 92, 48]}, {'caption': 'yellow, red, and grey helicopter', 'bbox': [304, 88, 56, 32]}, {'caption': 'yellow commercial aircraft', 'bbox': [808, 464, 76, 60]}, {'caption': 'black cargo aircraft', 'bbox': [948, 96, 44, 36]}, {'caption': 'yellow helicopter', 'bbox': [452, 108, 40, 36]}]  \n",
       "3  [{'caption': 'white and black light aircraft', 'bbox': [476, 324, 44, 44]}, {'caption': 'grey and black fighter plane', 'bbox': [760, 260, 56, 40]}, {'caption': 'yellow helicopter', 'bbox': [984, 500, 108, 44]}, {'caption': 'red fighter plane', 'bbox': [1016, 324, 208, 68]}, {'caption': 'yellow, red, and grey helicopter', 'bbox': [680, 340, 72, 44]}, {'caption': 'white missile', 'bbox': [1016, 176, 40, 48]}, {'caption': 'blue helicopter', 'bbox': [496, 512, 44, 40]}]  \n",
       "4                                                                                                                                                                                                                                                               [{'caption': 'white, red, and green fighter plane', 'bbox': [260, 232, 60, 56]}, {'caption': 'black camouflage fighter jet', 'bbox': [640, 172, 36, 32]}, {'caption': 'green light aircraft', 'bbox': [632, 328, 76, 48]}]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(os.path.join(\"advanced\", \"vlm.jsonl\"), lines=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cde90b4-0d31-48dc-971b-a0c322db9495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_splits = {}\n",
    "data_splits[\"train\"], data_splits[\"val\"] = random_split(data.to_dict(orient=\"index\"), [0.9, 0.1])\n",
    "# data_splits[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd478b51-2784-404e-994e-459f8aa8cc97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTextDualEncoderConfig {\n",
       "  \"logit_scale_init_value\": 2.6592,\n",
       "  \"model_type\": \"vision-text-dual-encoder\",\n",
       "  \"projection_dim\": 512,\n",
       "  \"text_config\": {\n",
       "    \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"RobertaForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 514,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"roberta\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"type_vocab_size\": 1,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 50265\n",
       "  },\n",
       "  \"transformers_version\": \"4.37.0\",\n",
       "  \"vision_config\": {\n",
       "    \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": null,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": null,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"dropout\": 0.0,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": null,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"quick_gelu\",\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"image_size\": 224,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_channels\": 3,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": null,\n",
       "    \"patch_size\": 32,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"projection_dim\": 512,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", \"FacebookAI/roberta-base\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d34452-3a29-420e-9e72-0b9ff044e166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms = {\n",
    "#     \"train\": T.Compose([\n",
    "#         T.Resize(config.vision_config.image_size),\n",
    "#         T.CenterCrop(config.vision_config.image_size),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(image_processor.image_mean, image_processor.image_std),\n",
    "#     ]),\n",
    "#     \"valid\": T.Compose([\n",
    "#         T.Resize(config.vision_config.image_size),\n",
    "#         T.CenterCrop(config.vision_config.image_size),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(image_processor.image_mean, image_processor.image_std),\n",
    "#     ])\n",
    "# }\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.SmallestMaxSize(config.vision_config.image_size),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "    A.Blur(blur_limit=(3, 5), p=0.3),\n",
    "    A.CenterCrop(height=config.vision_config.image_size, width=config.vision_config.image_size),\n",
    "    # A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0541a8a-236e-4c6b-966e-0e8fb7ee4a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop(row):\n",
    "    img = Image.open(os.path.join(\"advanced\", \"images\", row[\"image\"])).convert(\"RGB\")\n",
    "    bboxes = [anno[\"bbox\"] for anno in row[\"annotations\"]]\n",
    "    crop_imgs = [img.crop([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]) for bbox in bboxes]\n",
    "    \n",
    "    return crop_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c9143c3-f5fa-4f2b-b43c-c928241b6fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, transform=None):   \n",
    "        self.data = pd.DataFrame([row for row in data])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "        # extract features\n",
    "        self.crop_imgs = []\n",
    "        self.data.apply(lambda row: self.crop_imgs.extend(crop(row)), axis=1)\n",
    "        self.img_texts = []\n",
    "        self.data[\"annotations\"].apply(lambda annos: [self.img_texts.append(anno[\"caption\"]) for anno in annos])\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        # load image standardized to RGB and transform image\n",
    "        img = self.crop_imgs[idx]\n",
    "        text = self.img_texts[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=np.array(img))[\"image\"]\n",
    "            \n",
    "        text_inputs = self.tokenizer(text, padding=\"max_length\", truncation=True)\n",
    "        target = {\n",
    "            \"input_ids\": text_inputs.input_ids,\n",
    "            \"attention_mask\": text_inputs.attention_mask\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.crop_imgs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        \n",
    "        if worker_info is None:\n",
    "            return map(self.__getitem__, range(self.__len__()))\n",
    "\n",
    "        per_worker = int(math.ceil((self.__len__()) / float(worker_info.num_workers)))\n",
    "        worker_id = worker_info.id\n",
    "        iter_start = worker_id * per_worker\n",
    "        iter_end = min(iter_start + per_worker, self.__len__())\n",
    "        return map(self.__getitem__, range(iter_start, iter_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32ba17a-642f-4b55-a6e6-ec9f6bd269e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=36x36 at 0x7F5CCBED9D50>\n",
      "black camouflage fighter jet\n",
      "25179\n",
      "25179\n",
      "2734\n",
      "2734\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"train\": CLIPDataset(data_splits[\"train\"], tokenizer=tokenizer, transform=transform),\n",
    "    \"val\": CLIPDataset(data_splits[\"val\"], tokenizer=tokenizer, transform=transform)\n",
    "}\n",
    "print(datasets[\"train\"].crop_imgs[0])\n",
    "print(datasets[\"train\"].img_texts[0])\n",
    "print(len(datasets[\"train\"].crop_imgs))\n",
    "print(len(datasets[\"train\"].img_texts))\n",
    "print(len(datasets[\"val\"].crop_imgs))\n",
    "print(len(datasets[\"val\"].img_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b05e926-61c7-4150-8ee9-4e57f04709c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.8063,  0.8063,  0.8063,  ...,  0.8209,  0.8209,  0.8209],\n",
      "         [ 0.8063,  0.8063,  0.8063,  ...,  0.8209,  0.8209,  0.8209],\n",
      "         [ 0.8063,  0.8063,  0.8063,  ...,  0.8063,  0.8063,  0.8063],\n",
      "         ...,\n",
      "         [ 0.7041,  0.7187,  0.7187,  ..., -1.2959, -1.3105, -1.3105],\n",
      "         [ 0.6895,  0.6895,  0.7041,  ..., -1.2959, -1.3105, -1.3105],\n",
      "         [ 0.6749,  0.6749,  0.6895,  ..., -1.3105, -1.3105, -1.3105]],\n",
      "\n",
      "        [[ 1.0393,  1.0393,  1.0393,  ...,  1.0243,  1.0243,  1.0243],\n",
      "         [ 1.0393,  1.0393,  1.0393,  ...,  1.0243,  1.0243,  1.0243],\n",
      "         [ 1.0393,  1.0393,  1.0393,  ...,  1.0243,  1.0243,  1.0243],\n",
      "         ...,\n",
      "         [ 0.9343,  0.9343,  0.9493,  ..., -1.0918, -1.0918, -1.0918],\n",
      "         [ 0.9193,  0.9193,  0.9343,  ..., -1.0918, -1.0918, -1.0918],\n",
      "         [ 0.9043,  0.9043,  0.9043,  ..., -1.0918, -1.0918, -1.0918]],\n",
      "\n",
      "        [[ 1.3496,  1.3496,  1.3496,  ...,  1.3496,  1.3496,  1.3496],\n",
      "         [ 1.3496,  1.3496,  1.3496,  ...,  1.3496,  1.3496,  1.3496],\n",
      "         [ 1.3496,  1.3496,  1.3496,  ...,  1.3496,  1.3496,  1.3496],\n",
      "         ...,\n",
      "         [ 1.2643,  1.2643,  1.2785,  ..., -0.8119, -0.8119, -0.8119],\n",
      "         [ 1.2358,  1.2500,  1.2643,  ..., -0.8119, -0.8261, -0.8261],\n",
      "         [ 1.2216,  1.2216,  1.2358,  ..., -0.8119, -0.8261, -0.8403]]]), {'input_ids': [0, 14178, 32124, 7251, 4900, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b9ea5c-9106-4f3a-83af-06e6780da238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([img for img, target in batch])\n",
    "    input_ids = torch.tensor([target[\"input_ids\"] for img, target in batch], dtype=torch.long)\n",
    "    att_masks = torch.tensor([target[\"attention_mask\"] for img, target in batch], dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": imgs,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": att_masks,\n",
    "        \"return_loss\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d81638-e021-471e-8e41-15e02d721e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = \"models/clip\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "wd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7ad4905-9c72-4090-b547-923772616f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31480' max='31480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31480/31480 8:05:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.065024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.038983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.031773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.028868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.024279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.023101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.022984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.021620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.020464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.020674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.020595738664269447, 'eval_runtime': 92.8829, 'eval_samples_per_second': 29.435, 'eval_steps_per_second': 3.682, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/clip/best_model/preprocessor_config.json']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=wd,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"val\"],\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(os.path.join(out_dir, \"best_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(out_dir, \"best_model\"))\n",
    "image_processor.save_pretrained(os.path.join(out_dir, \"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ec8b9-bd3f-436a-84d4-a1c9a4da2383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
